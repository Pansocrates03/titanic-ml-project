# -*- coding: utf-8 -*-
"""Preprocessing..ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GVAsr0jObepyUUSSP83dQh-U8YCXlOd-
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

df = pd.read_csv("Titanic-Dataset.csv")
df.head()

df.info()

"""# **INGENIERIA DE FEATURES**

## 2.1 Creación de Variables Derivadas

### Variable Title
"""

# --- 1. VARIABLE: Title (TITULO) ---

def title_feature(df):
    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
    df['Title'] = df['Title'].replace('Mlle', 'Miss')
    df['Title'] = df['Title'].replace('Ms', 'Miss')
    df['Title'] = df['Title'].replace('Mme', 'Mrs')
    # ANÁLISIS DE LA RELACIÓN CON SUPERVIVENCIA
    print("RELACIÓN DE 'Title' CON LA SUPERVIVENCIA:")
    print(df.groupby('Title')['Survived'].mean().sort_values(ascending=False))
    # VERIFICACIÓN DE CALIDAD
    print("\nVERIFICACIÓN DE VALORES ÚNICOS Y CONTEO:")
    print(df['Title'].value_counts())
    return df

"""### Variable Family Size

"""

# --- 2. VARIABLE: FamilySize (TAMAÑO DE LA FAMILIA) ---

def family_size_feature(df):
    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
    # ANÁLISIS DE LA RELACIÓN CON SUPERVIVENCIA
    print("RELACIÓN DE 'FamilySize' CON LA SUPERVIVENCIA:")
    print(df.groupby('FamilySize')['Survived'].mean())
    # VERIFICACIÓN DE CALIDAD
    print("\nVERIFICACIÓN DE VALORES ÚNICOS Y CONTEO:")
    print(df['FamilySize'].value_counts().sort_index())
    return df

"""### Variable IsAlone"""

# --- 3. VARIABLE: IsAlone (ESTA SOLO) ---

def is_alone_feature(df):
    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)
    # ANÁLISIS DE LA RELACIÓN CON SUPERVIVENCIA
    print("RELACIÓN DE 'IsAlone' CON LA SUPERVIVENCIA:")
    print(df.groupby('IsAlone')['Survived'].mean())
    # VERIFICACIÓN DE CALIDAD
    print("\nVERIFICACIÓN DE VALORES ÚNICOS Y CONTEO:")
    print(df['IsAlone'].value_counts())
    return df

"""### Variable AgeGroup"""

# --- 4. VARIABLE: AgeGroup (GRUPO DE EDAD) ---
# MANEJO DE VALORES NULOS PARA LA EDAD

def age_group_feature(df):
      df['Age'].fillna(df['Age'].median(), inplace=True)
      bins = [0, 12, 18, 60, np.inf]
      labels = ['Child', 'Adolescent/Teenager', 'Adult', 'Senior']
      df['AgeGroup'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False)
      # ANÁLISIS DE LA RELACIÓN CON SUPERVIVENCIA
      print("RELACIÓN DE 'AgeGroup' CON LA SUPERVIVENCIA:")
      print(df.groupby('AgeGroup')['Survived'].mean().sort_values(ascending=False))
      # VERIFICACIÓN DE CALIDAD
      print("\nVERIFICACIÓN DE VALORES ÚNICOS Y CONTEO:")
      print(df['AgeGroup'].value_counts())
      return df

"""### Variable FarePerPerson"""

# --- 5. VARIABLE: FarePerPerson (TARIFA POR PERSONA) ---

def fare_per_person_feature(df):
    df['FarePerPerson'] = df['Fare'] / df['FamilySize']
    df['FarePerPerson'].fillna(df['FarePerPerson'].mean(), inplace=True) # Manejo de Nulos
    # ANÁLISIS DE LA RELACIÓN CON SUPERVIVENCIA (AGRUPANDO EN RANGOS)
    print("RELACIÓN DE 'FarePerPerson' CON LA SUPERVIVENCIA (POR RANGOS):")
    df['FarePerPerson_Group'] = pd.qcut(df['FarePerPerson'], 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])
    print(df.groupby('FarePerPerson_Group')['Survived'].mean().sort_values(ascending=False))
    # VERIFICACIÓN DE CALIDAD
    print("\nVERIFICACIÓN DE LA DISTRIBUCIÓN:")
    print(df['FarePerPerson'].describe())
    return df

"""### Variable Cabin Deck"""

# --- 6. VARIABLE: CabinDeck (CUBIERTA DE LA CABINA) ---

def cabin_deck_feature(df):
    df['CabinDeck'] = df['Cabin'].str.extract('([A-Z])', expand=False)
    df['CabinDeck'].fillna('Unknown', inplace=True)
    # ANÁLISIS DE LA RELACIÓN CON SUPERVIVENCIA
    print("RELACIÓN DE 'CabinDeck' CON LA SUPERVIVENCIA:")
    print(df.groupby('CabinDeck')['Survived'].mean().sort_values(ascending=False))
    # VERIFICACIÓN DE CALIDAD
    print("\nVERIFICACIÓN DE VALORES ÚNICOS Y CONTEO:")
    print(df['CabinDeck'].value_counts())
    return df

"""### Variable CabinKnown"""

# --- 7. VARIABLE: CabinKnown (CABINA CONOCIDA) ---

def cabin_known_feature(df):
  df['CabinKnown'] = df['Cabin'].isnull().astype(int)
  # ANÁLISIS DE LA RELACIÓN CON SUPERVIVENCIA
  print("RELACIÓN DE 'CabinKnown' CON LA SUPERVIVENCIA:")
  print(df.groupby('CabinKnown')['Survived'].mean())
  # VERIFICACIÓN DE CALIDAD
  print("\nVERIFICACIÓN DE VALORES ÚNICOS Y CONTEO:")
  print(df['CabinKnown'].value_counts())
  return df

"""### Variable TicketFrequency"""

# --- 8. VARIABLE: TicketFrequency (FREQUENCIA DE TICKEY ) ---

def ticket_frequency_feature(df):
  df['TicketFrequency'] = df.groupby('Ticket')['Ticket'].transform('count')
  # ANÁLISIS DE LA RELACIÓN CON SUPERVIVENCIA
  print("RELACIÓN DE 'TicketFrequency' CON LA SUPERVIVENCIA:")
  print(df.groupby('TicketFrequency')['Survived'].mean())
  # VERIFICACIÓN DE CALIDAD
  print("\nVERIFICACIÓN DE VALORES ÚNICOS Y CONTEO:")
  print(df['TicketFrequency'].value_counts().sort_index())
  return df

"""### Variable NameLength"""

# --- 9. VARIABLE: NameLength (LONGITUD DE NOMBRE ) ---

def name_length_feature(df):
  df['NameLength'] = df['Name'].str.len()
  # ANÁLISIS DE LA RELACIÓN CON SUPERVIVENCIA
  print("RELACIÓN DE 'NameLength' CON LA SUPERVIVENCIA:")
  print(df.groupby('NameLength')['Survived'].mean())
  # VERIFICACIÓN DE CALIDAD
  print("\nVERIFICACIÓN DE VALORES ÚNICOS Y CONTEO:")
  print(df['NameLength'].value_counts().sort_index())
  return df

"""### Variable  HasCabinNeighbor"""

# --- 10. VARIABLE: HasCabinNeighbor (CABINAS CERCANAS CON FAMILIARES ) ---

def has_cabin_neighbor_feature(df):
    df['HasCabinNeighbor'] = df['Cabin'].notnull().astype(int)
    # ANÁLISIS DE LA RELACIÓN CON SUPERVIVENCIA
    print("RELACIÓN DE 'HasCabinNeighbor' CON LA SUPERVIVENCIA:")
    print(df.groupby('HasCabinNeighbor')['Survived'].mean())
    # VERIFICACIÓN DE CALIDAD
    print("\nVERIFICACIÓN DE VALORES ÚNICOS Y CONTEO:")
    print(df['HasCabinNeighbor'].value_counts())
    return df

"""### Variable: TicketPrefix"""

# --- 11. VARIABLE: TicketPrefix (PREFIJO DEL TCIKET ) ---

def ticket_prefix_feature(df):
    df['TicketPrefix'] = df['Ticket'].str.extract(r'([A-Za-z]+)\d*')
    df['TicketPrefix'].fillna('Unknown', inplace=True)
    # ANÁLISIS DE LA RELACIÓN CON SUPERVIVENCIA
    print("RELACIÓN DE 'TicketPrefix' CON LA SUPERVIVENCIA:")
    print(df.groupby('TicketPrefix')['Survived'].mean().sort_values(ascending=False))
    # VERIFICACIÓN DE CALIDAD
    print("\nVERIFICACIÓN DE VALORES ÚNICOS Y CONTEO:")
    print(df['TicketPrefix'].value_counts())
    return df

# Crear una instancia del preprocesador
preprocessor = TitanicDatasetPreprocessor()

# Aplicar feature engineering al DataFrame
# Nota: fit_transform también realiza el feature engineering internamente
df_processed = preprocessor.fit_transform(df.copy())

# Convertir el resultado de fit_transform de regreso a un DataFrame (si es necesario)
# fit_transform devuelve un array numpy, necesitamos las columnas originales y las nuevas
# para crear un DataFrame con nombres de columna significativos.
# Como el objetivo es solo guardar el dataset con las nuevas variables creadas ANTES de las transformaciones
# del pipeline (escalado y one-hot encoding), aplicaremos solo la parte de feature engineering.

df_with_features = df.copy()
df_with_features = preprocessor._feature_engineering(df_with_features)


# Guardar el DataFrame con las nuevas variables en un archivo CSV
output_filename = "Titanic-Dataset_with_features.csv"
df_with_features.to_csv(output_filename, index=False)

print(f"Dataset con nuevas variables guardado en '{output_filename}'")

"""# 2.2 Transformaciones de Variables Existentes y 2.3 Análisis de Interacciones


"""

import numpy as np
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer

# La clase TitanicDatasetPreprocessor nos sirve para preprocesar los datos, ubicados en
# el dataframe del titanic llamado df, lo cual no sirve para alimentar modelos de
# machine learning a través de tres/3 funcionalidades que viene siendo:


# 1. Crear nuevas variables a partir de variables existentes (feature engineering)
# 2. Validar que estas variables estén correctas
# 3. Transformar los datos para que sean númericos y escalables para un modelo de Machine Learning




class TitanicDatasetPreprocessor:
    def __init__(self):
        self.pipeline = None # Con pipeline, podemos guardar el objeto de transformación que aplica el escalado
        self.feature_engineered = False # Con feature engineered, tenemos un flag que indica si las nuevas columnas/variables han sido creadas

    # Validación continua
    # Verificamos que la columna exista, checamos cuantos valores nulos hay, nos aseguramos que la columan sea el tipo de dato correcto
    # y checamos cuantos valores unicos hay.
    # Si no cumple ninguna de las condiciones entoncers nos marca error
    def _validate_feature(self, df, col, expected_dtype=None, max_unique=None):
        assert col in df.columns, f" Columna {col} no fue creada."
        assert df[col].isnull().mean() < 0.2, f" Columna {col} tiene demasiados valores nulos."
        if expected_dtype:
            assert df[col].dtype == expected_dtype, f" {col} debería ser {expected_dtype}, pero es {df[col].dtype}."
        if max_unique:
            assert df[col].nunique() <= max_unique, f" {col} tiene demasiados valores únicos ({df[col].nunique()})."
        print(f" Validación pasada: {col}")

    # Tratamiento de outliers con IQR (investigamos se encuuentran valores atípicos y/o extremos)
    def _treat_outliers(self, df, cols):
        for col in cols:
            if col in df.columns:
                Q1 = df[col].quantile(0.25) # Primer Cuartil
                Q3 = df[col].quantile(0.75) # Tercer Cuartil
                IQR = Q3 - Q1 # Intercuartil
                lower = Q1 - 1.5 * IQR # Limite Inferior
                upper = Q3 + 1.5 * IQR # Limmite Superior
                df[col] = np.clip(df[col], lower, upper) # Capturamos los outliers
                print(f" Outliers tratados en {col}")
        return df

    # Visualización de outliers
    def visualize_outliers(self, df, cols):
        for col in cols:
            if col in df.columns:
                fig, axes = plt.subplots(1, 2, figsize=(12, 4))

                # Boxplot para detectar outliers
                sns.boxplot(x=df[col], ax=axes[0])
                axes[0].set_title(f"Boxplot de {col}")

                # Histograma para ver distribución
                sns.histplot(df[col], bins=30, kde=True, ax=axes[1])
                axes[1].set_title(f"Distribución de {col}")

                plt.tight_layout()
                plt.show()

    # Feature Engineering
    def _feature_engineering(self, df):
        feature_funcs = [
            title_feature, family_size_feature, is_alone_feature, age_group_feature,
            fare_per_person_feature, cabin_deck_feature, cabin_known_feature,
            ticket_frequency_feature, name_length_feature, has_cabin_neighbor_feature,
            ticket_prefix_feature
        ]
        for func in feature_funcs:
            df = func(df)

        # Revisamos que la hayan creado ciertas features
        for col in ["Title", "FamilySize", "IsAlone", "AgeGroup", "FarePerPerson"]:
            self._validate_feature(df, col)

        # Agregamos las interraciones interacciones
        df = self._feature_interactions(df)
        return df

    # Interacciones
    def _feature_interactions(self, df):
        df["Sex*Class"] = df["Sex"].astype(str) + "_" + df["Pclass"].astype(str)
        df["Age*Class"] = pd.cut(df["Age"], bins=[0,12,18,40,60,80], labels=False) * df["Pclass"]
        df["Sex*AgeGroup"] = df["Sex"].astype(str) + "_" + df["AgeGroup"].astype(str)
        df["Fare*Embarked"] = pd.qcut(df["Fare"], 4, labels=False) * df["Embarked"].factorize()[0]
        df["Family*Class"] = df["FamilySize"] * df["Pclass"]

        # Validamos que se hayan creado lass interacciones de features
        for col in ["Sex*Class", "Age*Class", "Sex*AgeGroup", "Fare*Embarked", "Family*Class"]:
            self._validate_feature(df, col)

        return df

    # Fit - Aprende de los parámetros de Transformación
    def fit(self, X, y=None):
        X_proc = self._feature_engineering(X.copy())
        self.feature_engineered = True

        # Separamos todass las variables en dos listas: una lista de variables numéricas y otra de variables categóricas
        num_cols = X_proc.select_dtypes(include=np.number).columns.tolist()
        cat_cols = X_proc.select_dtypes(include="object").columns.tolist()

        # Rellenamos nulos con la mediana y luego escalamos los datos
        num_transformer = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler())
        ])

        # Rellenamos nulos con la moda y aplicamos One-Hot encoding
        cat_transformer = Pipeline(steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore"))
        ])

        # Nos aseguramoss de aplicar las transformaciones con sus respectivas columnas/variables
        self.pipeline = ColumnTransformer(transformers=[
            ("num", num_transformer, num_cols),
            ("cat", cat_transformer, cat_cols)
        ])

        self.pipeline.fit(X_proc)
        return self

    # Transform - Aplica las transformaciones a un nuevo dataset
    def transform(self, X):
        if not self.feature_engineered:
            raise RuntimeError("Primero usa fit() en datos de entrenamiento.")
        X_proc = self._feature_engineering(X.copy())
        return self.pipeline.transform(X_proc)


    # Combinación de Fit y de Transform
    def fit_transform(self, X, y=None):
        return self.fit(X, y).transform(X)